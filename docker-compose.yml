services:
  identifier:
    image: semtech/mu-identifier:1.10.3
    environment:
      SESSION_COOKIE_SECURE: "on"
    labels:
      - "logging=true"
  dispatcher:
    image: semtech/mu-dispatcher:2.1.0-beta.2
    volumes:
      - ./config/dispatcher:/config
    labels:
      - "logging=true"

  frontend:
    image: lblod/frontend-group-6-ai-help-me-ipdc
    labels:
      - "logging=true"

  # Our extra services here

  uuid-generation:
    image: redpencil/uuid-generation
    volumes:
      - ./config/uuid-generation/:/config
    environment:
      RUN_CRON_JOBS: "true" # our harvesting is without deltas right now

  embedding:
    image: lblod/group-6-ai-embedding-service

  structured-steps:
    image: lblod/group-6-ai-structured-steps-service # structured-steps:dev
    environment:
      # We run this in development to work around a temporary version
      # conflict with greenlet and langchain.  The dev mode resolves the
      # issue after build time.  This is not ok, but it does allow to
      # play with the end result without having Docker Compose
      # installed.
      MODE: "development"

  # Framework support

  resource:
    image: semtech/mu-cl-resources:1.25.0
    volumes:
      - ./config/resources:/config
    labels:
      - "logging=true"

  migrations:
    image: semtech/mu-migrations-service:0.9.0
    links:
      - triplestore:database
    environment:
      MU_SPARQL_TIMEOUT: 600
    volumes:
      - ./config/migrations:/data/migrations
    labels:
      - "logging=true"
  delta-notifier:
    image: semtech/mu-delta-notifier:0.4.0
    volumes:
      - ./config/delta:/config/
    labels:
      - "logging=true"
  database:
    image: semtech/sparql-parser:0.0.8
    volumes:
      - ./config/authorization/:/config/
      - ./data/authorization/:/data/
    labels:
      - "logging=true"
  triplestore:
    image: redpencil/virtuoso:1.2.0-rc.1
    environment:
      SPARQL_UPDATE: "true"
    volumes:
      - ./data/db:/data
      - ./config/virtuoso/virtuoso.ini:/data/virtuoso.ini
    labels:
      - "logging=true"

  # We run this as an infrastructure service.  We know we should be able
  # to combine these and/or embed it in a service.  At this point it
  # makes a lot of sense to share the resources though.

  # Thank you for the other hackathon team (we don't know the number)
  # for sharing!
  ollama:
    platform: linux/x86_64
    image: ollama/ollama:latest
    container_name: ollama
    volumes:
      - ./data/ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=-1          # Keep the model(s) loaded in memory
      - OLLAMA_HOST=0.0.0.0           # Listen on all interfaces
    healthcheck:
      test: "bash -c 'cat < /dev/null > /dev/tcp/localhost/11434'"
      interval: 2s                    # Check every 2 seconds
      timeout: 5s                     # Timeout after 5 seconds
      retries: 5                      # Max retries, if needed
      start_period: 5s                # Start checking after 5 seconds

  ollama-setup:
    image: curlimages/curl:7.77.0
    container_name: ollama-setup
    depends_on:
      ollama:
        condition: service_healthy    # Wait for 'ollama' container to be healthy
    entrypoint: >                     # Use the Ollama REST API to pull the mistral-nemo model
      /bin/sh -c 'curl http://ollama:11434/api/pull -d "{\"name\": \"llama3.2\"}" && echo "Curl command completed, model installed."'
 
